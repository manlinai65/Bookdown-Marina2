<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 Glossary 5 (Cluster Analysis) | Final Glossary Bookdown</title>
  <meta name="description" content="Final Glossary" />
  <meta name="generator" content="bookdown 0.19 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 Glossary 5 (Cluster Analysis) | Final Glossary Bookdown" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Final Glossary" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Glossary 5 (Cluster Analysis) | Final Glossary Bookdown" />
  
  <meta name="twitter:description" content="Final Glossary" />
  

<meta name="author" content="Marina Obata" />


<meta name="date" content="2020-06-17" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="glossary-4-mca-ca.html"/>

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Marina Obata final glossary</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Index for final Glossary</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#glossary-entries"><i class="fa fa-check"></i><b>1.1</b> 1.Glossary entries</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#glossary--pca-"><i class="fa fa-check"></i><b>1.2</b> 2.Glossary -PCA-</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#glossary--factor-analysis-"><i class="fa fa-check"></i><b>1.3</b> 3.Glossary -Factor Analysis-</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#glossary--mcaca-"><i class="fa fa-check"></i><b>1.4</b> 4.Glossary -MCA,CA-</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#glossary--cluster-analysis-"><i class="fa fa-check"></i><b>1.5</b> 5.Glossary -Cluster analysis-</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="glossary-entries-1.html"><a href="glossary-entries-1.html"><i class="fa fa-check"></i><b>2</b> Glossary entries</a><ul>
<li class="chapter" data-level="2.1" data-path="glossary-entries-1.html"><a href="glossary-entries-1.html#exploratory-data-analysis-eda"><i class="fa fa-check"></i><b>2.1</b> Exploratory Data Analysis (EDA)</a></li>
<li class="chapter" data-level="2.2" data-path="glossary-entries-1.html"><a href="glossary-entries-1.html#principal-component-analysis-pca"><i class="fa fa-check"></i><b>2.2</b> Principal Component Analysis (PCA)</a></li>
<li class="chapter" data-level="2.3" data-path="glossary-entries-1.html"><a href="glossary-entries-1.html#factor-analysis"><i class="fa fa-check"></i><b>2.3</b> Factor Analysis</a></li>
<li class="chapter" data-level="2.4" data-path="glossary-entries-1.html"><a href="glossary-entries-1.html#cluster-analysis"><i class="fa fa-check"></i><b>2.4</b> Cluster Analysis</a></li>
<li class="chapter" data-level="2.5" data-path="glossary-entries-1.html"><a href="glossary-entries-1.html#correspondence-analysis-ca"><i class="fa fa-check"></i><b>2.5</b> Correspondence Analysis (CA)</a></li>
<li class="chapter" data-level="2.6" data-path="glossary-entries-1.html"><a href="glossary-entries-1.html#eigenvalues"><i class="fa fa-check"></i><b>2.6</b> Eigenvalues</a></li>
<li class="chapter" data-level="2.7" data-path="glossary-entries-1.html"><a href="glossary-entries-1.html#eigenvectors"><i class="fa fa-check"></i><b>2.7</b> Eigenvectors</a></li>
<li class="chapter" data-level="2.8" data-path="glossary-entries-1.html"><a href="glossary-entries-1.html#variance"><i class="fa fa-check"></i><b>2.8</b> Variance</a></li>
<li class="chapter" data-level="2.9" data-path="glossary-entries-1.html"><a href="glossary-entries-1.html#covariance"><i class="fa fa-check"></i><b>2.9</b> Covariance</a></li>
<li class="chapter" data-level="2.10" data-path="glossary-entries-1.html"><a href="glossary-entries-1.html#references"><i class="fa fa-check"></i><b>2.10</b> References</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="glossary-2-pca.html"><a href="glossary-2-pca.html"><i class="fa fa-check"></i><b>3</b> Glossary 2 (PCA)</a><ul>
<li class="chapter" data-level="3.1" data-path="glossary-2-pca.html"><a href="glossary-2-pca.html#cloud-of-variables"><i class="fa fa-check"></i><b>3.1</b> Cloud of variables</a></li>
<li class="chapter" data-level="3.2" data-path="glossary-2-pca.html"><a href="glossary-2-pca.html#cloud-of-individuals"><i class="fa fa-check"></i><b>3.2</b> Cloud of individuals</a></li>
<li class="chapter" data-level="3.3" data-path="glossary-2-pca.html"><a href="glossary-2-pca.html#principal-component"><i class="fa fa-check"></i><b>3.3</b> Principal Component</a></li>
<li class="chapter" data-level="3.4" data-path="glossary-2-pca.html"><a href="glossary-2-pca.html#active-variables"><i class="fa fa-check"></i><b>3.4</b> Active variables</a></li>
<li class="chapter" data-level="3.5" data-path="glossary-2-pca.html"><a href="glossary-2-pca.html#supplementary-variables"><i class="fa fa-check"></i><b>3.5</b> Supplementary variables</a></li>
<li class="chapter" data-level="3.6" data-path="glossary-2-pca.html"><a href="glossary-2-pca.html#eigenvalues-interpretation-in-factor-analysis-and-principal-component-analysis"><i class="fa fa-check"></i><b>3.6</b> Eigenvalues (interpretation in Factor Analysis and Principal Component Analysis)</a></li>
<li class="chapter" data-level="3.7" data-path="glossary-2-pca.html"><a href="glossary-2-pca.html#rotation"><i class="fa fa-check"></i><b>3.7</b> Rotation</a></li>
<li class="chapter" data-level="3.8" data-path="glossary-2-pca.html"><a href="glossary-2-pca.html#screeplot"><i class="fa fa-check"></i><b>3.8</b> Screeplot</a></li>
<li class="chapter" data-level="3.9" data-path="glossary-2-pca.html"><a href="glossary-2-pca.html#references-1"><i class="fa fa-check"></i><b>3.9</b> References</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="glossary-3-factor-analysis.html"><a href="glossary-3-factor-analysis.html"><i class="fa fa-check"></i><b>4</b> Glossary 3 (Factor Analysis)</a><ul>
<li class="chapter" data-level="4.1" data-path="glossary-3-factor-analysis.html"><a href="glossary-3-factor-analysis.html#factor"><i class="fa fa-check"></i><b>4.1</b> Factor</a></li>
<li class="chapter" data-level="4.2" data-path="glossary-3-factor-analysis.html"><a href="glossary-3-factor-analysis.html#communalities"><i class="fa fa-check"></i><b>4.2</b> Communalities</a></li>
<li class="chapter" data-level="4.3" data-path="glossary-3-factor-analysis.html"><a href="glossary-3-factor-analysis.html#factor-loadings"><i class="fa fa-check"></i><b>4.3</b> Factor loadings</a></li>
<li class="chapter" data-level="4.4" data-path="glossary-3-factor-analysis.html"><a href="glossary-3-factor-analysis.html#factor-values-factor-scores"><i class="fa fa-check"></i><b>4.4</b> Factor values (Factor scores)</a></li>
<li class="chapter" data-level="4.5" data-path="glossary-3-factor-analysis.html"><a href="glossary-3-factor-analysis.html#anti-image-correlation-matrix"><i class="fa fa-check"></i><b>4.5</b> Anti-image correlation matrix</a></li>
<li class="chapter" data-level="4.6" data-path="glossary-3-factor-analysis.html"><a href="glossary-3-factor-analysis.html#kaiser-meyer-olkin-criterion-kmo"><i class="fa fa-check"></i><b>4.6</b> Kaiser-Meyer-Olkin criterion (KMO)</a></li>
<li class="chapter" data-level="4.7" data-path="glossary-3-factor-analysis.html"><a href="glossary-3-factor-analysis.html#measure-of-sampling-adequacy-msa"><i class="fa fa-check"></i><b>4.7</b> Measure of Sampling Adequacy (MSA)</a></li>
<li class="chapter" data-level="4.8" data-path="glossary-3-factor-analysis.html"><a href="glossary-3-factor-analysis.html#references-2"><i class="fa fa-check"></i><b>4.8</b> References</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="glossary-4-mca-ca.html"><a href="glossary-4-mca-ca.html"><i class="fa fa-check"></i><b>5</b> Glossary 4 (MCA, CA)</a><ul>
<li class="chapter" data-level="5.1" data-path="glossary-4-mca-ca.html"><a href="glossary-4-mca-ca.html#multiple-correspondence-analysis-vs-correspondence-analysis"><i class="fa fa-check"></i><b>5.1</b> Multiple Correspondence Analysis vs Correspondence Analysis</a></li>
<li class="chapter" data-level="5.2" data-path="glossary-4-mca-ca.html"><a href="glossary-4-mca-ca.html#cloud-of-individuals-in-mca"><i class="fa fa-check"></i><b>5.2</b> Cloud of individuals in MCA</a></li>
<li class="chapter" data-level="5.3" data-path="glossary-4-mca-ca.html"><a href="glossary-4-mca-ca.html#cloud-of-categories-in-mca"><i class="fa fa-check"></i><b>5.3</b> Cloud of categories in MCA</a></li>
<li class="chapter" data-level="5.4" data-path="glossary-4-mca-ca.html"><a href="glossary-4-mca-ca.html#proﬁle"><i class="fa fa-check"></i><b>5.4</b> Proﬁle</a></li>
<li class="chapter" data-level="5.5" data-path="glossary-4-mca-ca.html"><a href="glossary-4-mca-ca.html#weight"><i class="fa fa-check"></i><b>5.5</b> Weight</a></li>
<li class="chapter" data-level="5.6" data-path="glossary-4-mca-ca.html"><a href="glossary-4-mca-ca.html#inertia"><i class="fa fa-check"></i><b>5.6</b> Inertia</a></li>
<li class="chapter" data-level="5.7" data-path="glossary-4-mca-ca.html"><a href="glossary-4-mca-ca.html#chi-square-distance"><i class="fa fa-check"></i><b>5.7</b> Chi-Square-Distance</a></li>
<li class="chapter" data-level="5.8" data-path="glossary-4-mca-ca.html"><a href="glossary-4-mca-ca.html#contribution"><i class="fa fa-check"></i><b>5.8</b> Contribution</a></li>
<li class="chapter" data-level="5.9" data-path="glossary-4-mca-ca.html"><a href="glossary-4-mca-ca.html#supplementary-variablesindividuals-in-camca"><i class="fa fa-check"></i><b>5.9</b> Supplementary variables/individuals in CA/MCA</a></li>
<li class="chapter" data-level="5.10" data-path="glossary-4-mca-ca.html"><a href="glossary-4-mca-ca.html#concentration-ellipse"><i class="fa fa-check"></i><b>5.10</b> Concentration ellipse</a></li>
<li class="chapter" data-level="5.11" data-path="glossary-4-mca-ca.html"><a href="glossary-4-mca-ca.html#confidence-ellipse"><i class="fa fa-check"></i><b>5.11</b> Confidence ellipse</a></li>
<li class="chapter" data-level="5.12" data-path="glossary-4-mca-ca.html"><a href="glossary-4-mca-ca.html#binary-indicator-matrix"><i class="fa fa-check"></i><b>5.12</b> Binary indicator matrix</a></li>
<li class="chapter" data-level="5.13" data-path="glossary-4-mca-ca.html"><a href="glossary-4-mca-ca.html#burt-matrix"><i class="fa fa-check"></i><b>5.13</b> Burt matrix</a></li>
<li class="chapter" data-level="5.14" data-path="glossary-4-mca-ca.html"><a href="glossary-4-mca-ca.html#references-3"><i class="fa fa-check"></i><b>5.14</b> References</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="glossary-5-cluster-analysis.html"><a href="glossary-5-cluster-analysis.html"><i class="fa fa-check"></i><b>6</b> Glossary 5 (Cluster Analysis)</a><ul>
<li class="chapter" data-level="6.1" data-path="glossary-5-cluster-analysis.html"><a href="glossary-5-cluster-analysis.html#euclidean-distance"><i class="fa fa-check"></i><b>6.1</b> Euclidean distance</a></li>
<li class="chapter" data-level="6.2" data-path="glossary-5-cluster-analysis.html"><a href="glossary-5-cluster-analysis.html#manhattan-distance"><i class="fa fa-check"></i><b>6.2</b> Manhattan distance</a></li>
<li class="chapter" data-level="6.3" data-path="glossary-5-cluster-analysis.html"><a href="glossary-5-cluster-analysis.html#mahalanobis-distance"><i class="fa fa-check"></i><b>6.3</b> Mahalanobis distance</a></li>
<li class="chapter" data-level="6.4" data-path="glossary-5-cluster-analysis.html"><a href="glossary-5-cluster-analysis.html#hierarchical-clustering"><i class="fa fa-check"></i><b>6.4</b> Hierarchical clustering</a></li>
<li class="chapter" data-level="6.5" data-path="glossary-5-cluster-analysis.html"><a href="glossary-5-cluster-analysis.html#partitioning"><i class="fa fa-check"></i><b>6.5</b> Partitioning</a></li>
<li class="chapter" data-level="6.6" data-path="glossary-5-cluster-analysis.html"><a href="glossary-5-cluster-analysis.html#dendrogram"><i class="fa fa-check"></i><b>6.6</b> Dendrogram</a></li>
<li class="chapter" data-level="6.7" data-path="glossary-5-cluster-analysis.html"><a href="glossary-5-cluster-analysis.html#references-4"><i class="fa fa-check"></i><b>6.7</b> References</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Final Glossary Bookdown</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="glossary-5-cluster-analysis" class="section level1">
<h1><span class="header-section-number">Chapter 6</span> Glossary 5 (Cluster Analysis)</h1>
<div id="euclidean-distance" class="section level2">
<h2><span class="header-section-number">6.1</span> Euclidean distance</h2>
<p>In cluster analysis, the proximities between individuals are quantified by distance measures, Euclidean distance measure is the most commonly used one, it used to assign points to clusters. It treats each variable as equally important when calculating the distance, usually computed from raw data, not from standardized data. In order to calculate the Euclidean distance, the sum of squared distances of adjoining side(B), opposite side(A) of the hypotenuse(C) are square rooted (sqrt(A^2 + B^2), see the image below). If Euclidean distance is chosen, the observations with high values of features will be clustered together, so will be for the observations with low values of features.</p>
<div class="figure">
<img src="Euclidean.png" alt="© Tryfos, 2001" style="width:50.0%" />
<p class="caption">© Tryfos, 2001</p>
</div>
<p>Following matrix table shows an example of Euclidean distance from the data of arrested record in the U.S.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb22-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">123</span>)</a>
<a class="sourceLine" id="cb22-2" data-line-number="2">ss &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">50</span>, <span class="dv">15</span>)</a>
<a class="sourceLine" id="cb22-3" data-line-number="3">df5 &lt;-<span class="st"> </span>USArrests[ss, ] </a>
<a class="sourceLine" id="cb22-4" data-line-number="4">df.scaled &lt;-<span class="st"> </span><span class="kw">scale</span>(df5) </a>
<a class="sourceLine" id="cb22-5" data-line-number="5">dist.eucl &lt;-<span class="st"> </span><span class="kw">dist</span>(df.scaled, <span class="dt">method =</span> <span class="st">&quot;euclidean&quot;</span>)</a>
<a class="sourceLine" id="cb22-6" data-line-number="6"><span class="kw">round</span>(<span class="kw">as.matrix</span>(dist.eucl)[<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>, <span class="dv">1</span><span class="op">:</span><span class="dv">3</span>], <span class="dv">1</span>)</a></code></pre></div>
<pre><code>##            New Mexico Iowa Indiana
## New Mexico        0.0  4.1     2.5
## Iowa              4.1  0.0     1.8
## Indiana           2.5  1.8     0.0</code></pre>
</div>
<div id="manhattan-distance" class="section level2">
<h2><span class="header-section-number">6.2</span> Manhattan distance</h2>
<p>Manhattan distance captures the distance by aggregating the pairwise absolute difference between each variable. It follows a route along the non-hypotenuse sides of a triangle. The name of Manhattan is referring the grid-like layout of most American cities in which it is hard to go directly between two points. The difference between Euclidean distance is that it is not using the squared distance, just summing the absolute numbers(see the image below). Manhattan distance is less sensitive to outliers when applied to cluster analysis.</p>
<p><span class="math display">\[
d(i.j) = \sum|Xik-Xjk|
\]</span></p>
<div class="figure">
<img src="Manhattan.png" alt="© Chatterjee, 2019" style="width:40.0%" />
<p class="caption">© Chatterjee, 2019</p>
</div>
<p>Following matrix table shows an example of Euclidean distance. As you can see, even though the used data is the same, it shows different values from Euclidean distance, having larger distances.</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb24-1" data-line-number="1">dist.manh &lt;-<span class="st"> </span><span class="kw">dist</span>(df.scaled, <span class="dt">method =</span> <span class="st">&quot;manhattan&quot;</span>)</a>
<a class="sourceLine" id="cb24-2" data-line-number="2"><span class="kw">round</span>(<span class="kw">as.matrix</span>(dist.manh)[<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>, <span class="dv">1</span><span class="op">:</span><span class="dv">3</span>], <span class="dv">1</span>)</a></code></pre></div>
<pre><code>##            New Mexico Iowa Indiana
## New Mexico        0.0  7.8     4.4
## Iowa              7.8  0.0     3.4
## Indiana           4.4  3.4     0.0</code></pre>
</div>
<div id="mahalanobis-distance" class="section level2">
<h2><span class="header-section-number">6.3</span> Mahalanobis distance</h2>
<p>The Mahalanobis distance is an approach to scale the distance in accordance with the variability of each variable. In the K-means algorithm, the Mahalanobis distance metric could be used to capture the variance structure of the clusters, it allows K-means to identify non-homogeneous clusters. When correlations between variables within groups are small, Mahalanobis distance will be similar as squared Euclidean distance. Which means, Mahalanobis distance increases when the distance of centers between two groups increases, it decreases when there are larger variations within the group. It is suggested by Kurczynski(1969) to adapt the generalized Mahalanobis distance with categorical variables.</p>
<div class="figure">
<img src="Mahala.png" alt="© Chatterjee, 2019" style="width:40.0%" />
<p class="caption">© Chatterjee, 2019</p>
</div>
<p>The following calculation is showing the mahalanobis distance from the same data of arrested record in the U.S.</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb26-1" data-line-number="1">dist.mahala &lt;-<span class="st"> </span><span class="kw">distances</span>(df.scaled, <span class="dt">normalize =</span> <span class="st">&quot;mahalanobize&quot;</span>)</a>
<a class="sourceLine" id="cb26-2" data-line-number="2">df.n &lt;-<span class="st"> </span><span class="kw">round</span>(<span class="kw">as.matrix</span>(dist.mahala)[<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>, <span class="dv">1</span><span class="op">:</span><span class="dv">3</span>], <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb26-3" data-line-number="3"><span class="kw">rownames</span>(df.n) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;New Mexico&quot;</span>, <span class="st">&quot;Iowa&quot;</span>, <span class="st">&quot;Indiana&quot;</span>)</a>
<a class="sourceLine" id="cb26-4" data-line-number="4"><span class="kw">colnames</span>(df.n) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;New Mexico&quot;</span>, <span class="st">&quot;Iowa&quot;</span>, <span class="st">&quot;Indiana&quot;</span>)</a>
<a class="sourceLine" id="cb26-5" data-line-number="5">df.n </a></code></pre></div>
<pre><code>##            New Mexico Iowa Indiana
## New Mexico        0.0  2.9     2.5
## Iowa              2.9  0.0     1.6
## Indiana           2.5  1.6     0.0</code></pre>
<p>As it is mentioned, it is closer to Euclidean distance than Manhattan distance in this example.</p>
</div>
<div id="hierarchical-clustering" class="section level2">
<h2><span class="header-section-number">6.4</span> Hierarchical clustering</h2>
<p>Hierarchical clustering is one of the main categories of clustering techniques, it concerns a suitable choice of a distance function to express the distance and patterns. It is a statistical method for finding relatively homogeneous clusters, based on distances between objects. It reduces the number of clusters by combining the clusters having similar characteristics at each level till reach to only one cluster left (see the image below).</p>
<div class="figure">
<img src="hierarchical.png" alt="© Sinharay, 2010" style="width:40.0%" />
<p class="caption">© Sinharay, 2010</p>
</div>
</div>
<div id="partitioning" class="section level2">
<h2><span class="header-section-number">6.5</span> Partitioning</h2>
<p>Partitioning is a process in cluster analysis, grouping the similar observations into homogeneous subsets. For example, partitioning in hierarchical analysis means the process to combining the similar clusters at each level.
The Partitioning-based clustering is one of the major types of cluster analysis, Grabmeier and Rudolph’s (2002) taxonomy separated partitioning methods from hierarchical methods. This method is based on interactive relocation of data points between clusters, the quality is measured by a clustering criterion. There are various types of partitioning clustering algorithms such as K-means, K-methods, FCM, CLARANS.
Among that, K-means requires the analyst to define K number of clusters before running the algorithm.</p>
</div>
<div id="dendrogram" class="section level2">
<h2><span class="header-section-number">6.6</span> Dendrogram</h2>
<p>Dendrogram is a type of tree shaped diagram showing the hierarchical clustering, it can visualize how close/far the clusters are to other clusters(see the figure below). Those clades with different heights are indicating how similar/dissimilar the clusters are, having same heights of clades meaning that they are similar to each other. Also, dendrogram is illustrating the process of divisions which have been made at each level of hierarchical analysis, it enables to decide the level at which to cut the tree for suitable number of clustering groups.
Following dendrogram shows the example data of arrested records in the U.S.</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb28-1" data-line-number="1">res.dist &lt;-<span class="st"> </span><span class="kw">dist</span>(df5, <span class="dt">method =</span> <span class="st">&quot;euclidean&quot;</span>)</a>
<a class="sourceLine" id="cb28-2" data-line-number="2">res.hc &lt;-<span class="st"> </span><span class="kw">hclust</span>(<span class="dt">d =</span> res.dist, <span class="dt">method =</span> <span class="st">&quot;ward.D2&quot;</span>)</a>
<a class="sourceLine" id="cb28-3" data-line-number="3"><span class="kw">fviz_dend</span>(res.hc, <span class="dt">cex =</span> <span class="fl">0.5</span>)</a></code></pre></div>
<p><img src="Bookdown-MarinaObata2_files/figure-html/5-5-1.png" width="60%" /></p>
</div>
<div id="references-4" class="section level2">
<h2><span class="header-section-number">6.7</span> References</h2>
<p>Chatterjee.D.R.(2019).Log Book — Guide to Distance Measuring Approaches for K- Means Clustering. Towards data science. [online].Retrieved from <a href="https://towardsdatascience.com/log-book-guide-to-distance-measuring-approaches-for-k-means-clustering-f137807e8e21" class="uri">https://towardsdatascience.com/log-book-guide-to-distance-measuring-approaches-for-k-means-clustering-f137807e8e21</a> [accessed on 15.06.2020].</p>
<p>Deltaflair Team (2019). Clustering in R – A Survival Guide on Cluster Analysis in R for Beginners[online].Retrieved from <a href="https://data-flair.training/blogs/clustering-in-r-tutorial/" class="uri">https://data-flair.training/blogs/clustering-in-r-tutorial/</a> [accessed on 15.06.2020].</p>
<p>Kassambara.A. (2017).Articles - Cluster Analysis in R: Practical Guide. Essentials. Statistical tool for high-thoughtput data analysis. [online]. Retrieved from <a href="http://www.sthda.com/english/articles/25-clusteranalysis-in-r-practical-guide/" class="uri">http://www.sthda.com/english/articles/25-clusteranalysis-in-r-practical-guide/</a> [accessed on 15.06.2020].</p>
<p>Kassambara.A. (2017). Practical Guide To Cluster Analysis in R. sthda.com . Edition 1.</p>
<p>Kassambara.A. (2017).Articles - Principal Component Methods in R: Practical Guide. Principal Component Analysis in R: prcomp vs princomp. [online].Retrieved from <a href="http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/118-principal-component-analysis-in-r-prcomp-vs-princomp/" class="uri">http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/118-principal-component-analysis-in-r-prcomp-vs-princomp/</a> [accessed on 15.06.2020].</p>
<p>Le Roux.B.and Rouanet.H. (2004). Geometric Data Analysis. Kluwer Academic Publishers.</p>
<p>Nalson.J.(2012). ON K-means Clustering Using Mahalanobis Distance[onlone]. Retrieved from <a href="https://pdfs.semanticscholar.org/b029/5854310ef3e35a0d71bd73554840e38a5bd8.pdf" class="uri">https://pdfs.semanticscholar.org/b029/5854310ef3e35a0d71bd73554840e38a5bd8.pdf</a> [accessed on 16.06.2020].</p>
<p>Savje.F. (2019). Package ‘distances’. CRAN.[online].Retrieved from <a href="https://cran.r-project.org/web/packages/distances/distances.pdf" class="uri">https://cran.r-project.org/web/packages/distances/distances.pdf</a> [accessed on 15.06.2020].
Sinharay.S.(2010).International Encyclopedia of Education (Third Edition).Cluster Analysis.</p>
<p>Tryfos.P.(1997). Chapter 15 Cluster analysis.[online].Retrieved from <a href="http://www.yorku.ca/ptryfos/f1500.pdf" class="uri">http://www.yorku.ca/ptryfos/f1500.pdf</a> [accessed on 15.06.2020].</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="glossary-4-mca-ca.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Bookdown-MarinaObata2.pdf", "Bookdown-MarinaObata2.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
